{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech Recognition \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Feature Extraction\n",
    "2. Acoustic Modeling\n",
    "3. Language Modeling\n",
    "4. Decoding\n",
    "5. Research in Speech Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taxconomy\n",
    "1. Speaker\n",
    "    - speaker dependent\n",
    "    - speaker independent\n",
    "\n",
    "\n",
    "2. Voca\n",
    "    - small, digit recognition\n",
    "    - large 1000,  dictation\n",
    "\n",
    "\n",
    "3. Fluency\n",
    "    - Isolated word (command and control)\n",
    "    - continuous speech (connected words)\n",
    "        - read speech (dictation) 문법에 맞는 이야기 \n",
    "        - conversational speech (closed caption) 대화 \n",
    "\n",
    "\n",
    "4. Noise\n",
    "    - Quiet envirnments (close-talking)\n",
    "    - Noisy envirnments (distant-talking)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전 소리가 입력으로 들어왔을 때 다음 입력이 이러한 단어가 될 확률이 최대인 것을 구하는 방식으로 음성 인식이 이루어진다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.researchgate.net/profile/Jozef_Juhar/publication/285207720/figure/fig1/AS:391606273363981@1470377619626/Block-diagram-of-a-general-LVCSR-system.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![pnd](http://tcpschool.com/examples/images/deep_089.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Feature Extraction\n",
    "### MFCC\n",
    "- 50~12만개의 음소로 나눌 수 있다. \n",
    "- 인간의 소리 인식과 소리 생성 모델을 본따 만들었다. \n",
    "\n",
    "\n",
    "1. preemphasis\n",
    "2. Windowing (Hamming windowing)\n",
    "   - 20-30 milliseconds in length\n",
    "   - 10 miilisenconds interval \n",
    "   - 가운데는 커지고 끝은 작아진다. 끊어도 상관없도록\n",
    "   - overlap이 되도록\n",
    "3. DFT\n",
    "    - 푸리에 변환: 시간별로 정도(파형)를 주파수 별로 얼마나 많은 에너지가 있는지를 찾는다. (스펙트럼으로 변환)\n",
    "    - 각각의 스펙트럼을 매 interval 마다 적어준다. \n",
    "    - 스펙트럼 그램을 만든다. \n",
    "4. Mel Frequency filter\n",
    "    - Frequency masking으로 고주파가 나오면 뒤에 있는 주파수의 변화를 감지할 수 없다. \n",
    "    - Critical bands \n",
    "    - 고주파는 넓게, 저주파는 좁게 \n",
    "5. Logarithm\n",
    "    - Loudness is percived in logarithmic scale. \n",
    "6. IDFT\n",
    "    - 사람의 목소리의 주파수는 필요없고 혀와 입의 모양이 필요하다.\n",
    "    - 입을 앞으로 내밀면 관이 길어져서 저주파의 소리가 잘 나온다. \n",
    "    - G는 일정하지만 H에 따라 달라진다. 원래의 형태에 H를 곱해지고 그것을 Log취하면 H가 더해진다. (Source=G, filter=H) \n",
    "    - Source = G는 제거\n",
    "7. Derivations\n",
    "    - 1차 미분, 2차 미분 값을 더해서 넣는다.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 음향 모델\n",
    "### HMM\n",
    "How to make HMM: https://github.com/ananthpn/pyhmm\n",
    "\n",
    "\n",
    "https://librosa.github.io/librosa/generated/librosa.feature.mfcc.html\n",
    "\n",
    "\n",
    "- Monophones = 음소(어떤 음소가 이러한 소리가 나온다.)\n",
    "\n",
    "\n",
    "1. (앞면, 뒷면)state possibility: emission possibility\n",
    "2. 상태 전이 확률: transmission possibility\n",
    "\n",
    "\n",
    "- 앞면이나 뒷면의 정보를 통해서 몇 번 동전인지를 맞추는 것 (확률로 예측)\n",
    "\n",
    "\n",
    "계속적으로 parameter를 업데이트 \n",
    "\n",
    "\n",
    "#### 3 questions in HMM\n",
    "1. Forward algorithm, 후향 알고리즘\n",
    "2. 비터비 알고리즘 \n",
    "3. 바움-웰치 알고리즘, EM(Expectation Maximization) 알고리즘\n",
    "\n",
    "\n",
    "#### 왜 HMM을 사용하는가\n",
    "- 매번 음성의 길이가 다르기 때문에 비슷한지를 봐야한다. \n",
    "    - Time wapping: 예전에 했던 방식 \n",
    "- 하지만 HMM은 음소단위로 그 음소가 가지는 스펙트럼 \n",
    "    - '아' 라고 발음하더라도 처음 시작과 끝 부분이 다르다. '간' 과 '악' 중간에는 stable \n",
    "    - 이러한 스펙트럼 백터가 결국 상태의 확률이 들어있다. (이 음소가 나올 확률)\n",
    "    - 어떤 state에서 나왔는지 모르지만 이 발음이 나올 확률을 구해준다.\n",
    "- 벡터의 종류\n",
    "    1. 실수이므로 연속확률분포를 통해서 __GMM__을 통해서 확률을 구한다. \n",
    "    2. 이차원을 가중치값을 주어서 확률을 구한다. \n",
    "\n",
    "\n",
    "### Artificial Neural Networks\n",
    "- 자극이 어느 정도 이상이 되었을 때 출력을 한다. \n",
    "- Bolean  \n",
    "- Bolean XOR의 문제는 일차 부등식을 2개로 만들면 문제를 해결할 수 있다. \n",
    "- 이렇게 여러 층으로 인지프로그램을 만든다. Multi perception\n",
    "\n",
    "\n",
    "#### Error Backpropagation Learning\n",
    "1. 가중치 값을 찾을 수 있게 된다. \n",
    "2. 층을 많이 쌓을 수록 미분의 값이 작은 부분을 찾기가 어려워진다. \n",
    "\n",
    "\n",
    "#### Deep Neural Networks and Deep Learning\n",
    "1. 아무리 층을 많이 쌓아도 학습할 수 있는 알고리증을 만들었다. \n",
    "2. DNN\n",
    "    1. CNN(이미지): static\n",
    "    2. RNN(입력이 밑으로 갈 수 있다. self transition이 가능하다.): sequence\n",
    "        1. 단기기억장치를 만들 수 있다. \n",
    "        2. LSTM \n",
    "3. GMM이 아니라 DNN이 만들어낸다. (DNN-HMM) \n",
    "\n",
    "\n",
    "#### Triphones\n",
    "좌우에 뭐가 있느냐에 따라서 다른 단어로 인식한다.\n",
    "\n",
    "\n",
    "1. word internal Triphones\n",
    "2. Cross-word Triphones\n",
    "\n",
    "-> 너무 상태가 구체화되어있기 때문에 __State Clustering__\n",
    "\n",
    "\n",
    "-> monophone도 아니고 triphone도 아닌 중간 단계: __Decision Tree Based State Clustering __\n",
    "\n",
    "\n",
    "### Emvedded Traning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 언어 모델\n",
    "### SLM (n-gram models) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding\n",
    "### Engine \n",
    "- search를 어떻게 하느냐 \n",
    "\n",
    "\n",
    "word HMM Construction\n",
    "\n",
    "\n",
    "#### s + eh + ... + 이렇게 해서 n개의 voca를 만들 수 있다. 전향 알고리즘 이용 \n",
    "\n",
    "\n",
    "__word HMM Construction with silence__\n",
    "\n",
    "\n",
    "#### n 개의 단어 중에서 무엇이 제일 확률이 높은지 비터비 알고리즘을 이용한다.\n",
    "\n",
    "\n",
    "Weighted Finite State Transducers: 비터비 알고리즘 이용 \n",
    "\n",
    "\n",
    "11 개의 단어는 0.5% Error 이므로 우리가 이걸 목표로 한다. 가능하면 20.000의 단어도 3%의 Error Rate가 가능하다. \n",
    "\n",
    "\n",
    "딥러닝으로 한 것은 \n",
    "\n",
    "\n",
    "#### End to End Speech Recognintion\n",
    "- spectrum의 input을 넣으면 단어를 output이 나오도록 한다. \n",
    "- 10개 ouput 10 이지만 input 과 output의 결과 길이가 다르다. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# TTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Generation\n",
    "#### Generative Adversarial Networks\n",
    "- Random Noise를 넣으면 출력이 이미지로 원래의 분류기와는 반대의 과정을 학습시킨다.\n",
    "- 새로운 이미지를 생성하는 neural network를 만들었다. \n",
    "- 존재하지 않는 사람을 만들어낸다. \n",
    "\n",
    "\n",
    "-> 이를 통해서 소리를 만들 수 있게 된다. (wave net)\n",
    "\n",
    "### Image Tranfer\n",
    "#### CycleGAN\n",
    "Cycle Consistency를 통해서 내가 말을 한 내용을 누군가의 목소리로 변환을 할 수 있다. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 기본적인 인공지능의 지식\n",
    "1. Machine Learning\n",
    "    - Deep learning\n",
    "    - 여러 분야들이 있다. \n",
    "2. AI \n",
    "    - 로봇이 사람처럼 합리적으로 생각하고 행동하는 것 \n",
    "    - AI의 분야 중에서 Machine Learning이 있다. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
