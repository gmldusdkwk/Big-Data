{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network\n",
    "상황에 따라 가능성이 나오게 된다. \n",
    "- 1에 해당하는 값이 0.8이므로 이는 1일 가능성이 높다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function\n",
    "- sum((Prediction - Target)^2) = mean squared error가 적을수록 neural network를 최소화한다. \n",
    "- regression:\n",
    "    1. x와 y의 관계를 통해서 y를 예측 할 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification\n",
    "1. Sigmoid Cross Entropy Loss\n",
    "    - cross entropy를 구한만큼 둘의 크기가 다르다.\n",
    "    - 0, 1의 결과만을 가지고 있을 경우 이용\n",
    "2. Softmax Cross Entropy Loss\n",
    "    - 여러 개의 entropy가 존재할 때 이용 \n",
    "\n",
    "### Entropy\n",
    "1. (-sum(pi(logpi))) = p의 확률분포의 엔트로피: H(P)\n",
    "2. (-sum(qi(logqi))) = q의 확률분포의 엔트로피: H(Q)\n",
    "3. Cross Entropy H(P:Q) = (-sum(pi(logqi)))\n",
    "\n",
    "\n",
    "이것을 pi를 yi(실제 확률분포)로 qi를 yi의 hat(예측 확률분포)으로 변경"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BackPropagation\n",
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Activation Functions](https://ratsgo.github.io/deep%20learning/2017/04/22/NNtricks/)\n",
    "- 입력 신호의 총합을 출력신호로 변환하는 함수\n",
    "- 비선형함수\n",
    "\n",
    "\n",
    "\n",
    "## Sigmoid\n",
    "![sigmoid](http://i.imgur.com/HpSpWal.png)\n",
    "- 완전히 값을 전달하지 않거나(0) 혹은 완전히 전달된다(1)는 특성\n",
    "- 'firing rate' \n",
    "![](http://i.imgur.com/WpKD6kW.png)\n",
    "- 시그모이드 함수를 1차 미분한 그래프: 최대는 0.25\n",
    "\n",
    "\n",
    "1.  gradient 'kill'\n",
    "    - 뉴런의 활성값이 0 또는 1에 매우 가깝다면 해당 편미분 값이 0에 매우 가까워지는 특성\n",
    "    - 학습의 결과가 back propagation 과정에서 전달되지 못하고 이에 따라 weight 값의 조정이 되지 않습니다. \n",
    "2. Sigmoid outputs are not zero-centered\n",
    "    - Sigmoid functio의 결과값은 그 중점이 0이 아니며, 모두 양수이다. \n",
    "    - 모수를 추정하는 학습이 어렵다는 단점이 있다. \n",
    "3. exp() is a bit expensive.\n",
    "    - 지그재그 path만 가능하다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tanh\n",
    "![tanh](http://i.imgur.com/xaQpDt4.png)\n",
    "- 시그모이드 함수의 크기와 위치를 조절한 함수\n",
    "![graident](http://i.imgur.com/0mVuW9h.png)\n",
    "\n",
    "\n",
    "1. non-linear function\n",
    "2. zero-centered: 지그재그 path 가 없어진다.\n",
    "3. still kills gradients when saturated :("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU\n",
    "1. non-linear function\n",
    "2. 미분하기가 편리하여 계산 복잡성이 낮다.\n",
    "3. Not zero-centered output \n",
    "4. 여러 개로 쌓아서 복잡한 함수를 나타낼 수 있다.\n",
    "    - deep\n",
    "    - wide: ReLU를 여러 개로 함수를 approximation을 할 수 있다. \n",
    "\n",
    "### 딥러닝의 알고리즘 중에 이것을 왜 쓰는지 알아야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization methods\n",
    "![](1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Gradient Descent\n",
    "![](https://engmrk.com/wp-content/uploads/2018/04/Fig2.png)\n",
    "![](https://engmrk.com/wp-content/uploads/2018/04/Fig3.png)\n",
    "1. Batch Gradient Descent\n",
    "2. Stochastic Gradient Descent\n",
    "    - Local minima에 갇히게 된다.\n",
    "3. Mini-Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Momentum Update\n",
    "![](http://i.imgur.com/6nnAWF8.png)\n",
    "- Avoiding Minima\n",
    "- Smoother Iteration\n",
    "- 조그마한 언덕이 있다면 넘어서 갈 수 있어서 local mean에 갇히지 않게 된다. \n",
    "- global optimum으로 갈 수 있게 된다.\n",
    "- 초기의 momentum(v)를 0으로 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. AdaGrad\n",
    "- 학습률 감소와 연관된 기법\n",
    "- 각각의 학습 파라미터에 맞춤형으로 학습률을 조정하면서 학습을 진행한다.\n",
    "- 각각으로 값이 계속 증가하여 무조건 0 이 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. RMSProp\n",
    "- Exponential Moving Average(지수 이동 평균) 이용 \n",
    "- 과거의 모든 기울기를 다 더해 균일하게 반영하는 것이 아니라, 먼 과거의 기울기는 서서히 잊고 새로운 기울기 정보를 크게 반영하기 위해서 h를 계산할 때 지수이동평균 이용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Adam\n",
    "- Momenum + RMSprop\n",
    "- 최근에 제일 많이 이용 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Batch Normalization](https://shuuki4.wordpress.com/2016/01/13/batch-normalization-%EC%84%A4%EB%AA%85-%EB%B0%8F-%EA%B5%AC%ED%98%84/)\n",
    "- 기본적으노 Gradient Vanishing/Gradient Exploding이 일어나지 않도록 하는 아이디어 중의 하나\n",
    "- Internal Covariance Shift라는 현상은 Network의 각 층이나 Activation마다 input의 distribution이 달라지는 현상을 의미한다.\n",
    "- 이 현상을 막기 위해서 간단하게 각 층의 input의 distibution을 평균 0, 표준편차 1인 input으로 normalize 시키는 방법\n",
    "\n",
    "\n",
    "실제로 이 Batch Normalization을 네트워케 적용시킬 때는, 특정 hidden Layer에 들어가기 전에 Batch Normalization을 더해주어 input을 modify해준 뒤 새로운 값을 activation function으로 넣어주는 방식으로 한다. bias의 역할을 위해서 마지막에 더해야 한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble and Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
