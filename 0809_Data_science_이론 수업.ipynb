{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "고려대학교 수업 8월 9일 \n",
    "\n",
    "# Data Science\n",
    "***\n",
    "\n",
    "![Data Science](https://static1.squarespace.com/static/5150aec6e4b0e340ec52710a/t/51525c33e4b0b3e0d10f77ab/1364352052403/Data_Science_VD.png)\n",
    "\n",
    "-> Programmer + statistic + domain science\n",
    "\n",
    "Renaissance 화학, 돈의 유동성을 자연 과학 부분에서 슈퍼컴퓨터로 모델링 해서 투자, 수학, 컴퓨터 과학, 화학, \n",
    "\n",
    "### Movie Rating \n",
    "https://www.imdb.com/\n",
    "1. Predict movie rating\n",
    "2. What does the social network of actors look like (six degrees of Kevin Bacon)\n",
    "3. Predict the popularity of movie\n",
    "https://trends.google.com/trends/?geo=US\n",
    "\n",
    "## Data Munging\n",
    "### Cleaning preparing \n",
    "Data \n",
    "1. Ask an interesting \n",
    "\n",
    "\n",
    "2. get\n",
    "    1. 내가 데이터 진흥원 수업에서 배운 것처럼 여러 곳에서 데이터를 얻을 수 있다. \n",
    "    2. Merge!: Unit conversion, DB 설계에 있어서 index가 중요하다.\n",
    "    3. Missing data\n",
    "    4. Outlier: visualization - system적인 사고로 Fix\n",
    "    5. normalization: mean = 0 std = 1\n",
    "    6. Cleaning \n",
    "    7. 정규분포, T분포, 통계적 가설 검증, \n",
    "    \n",
    "    \n",
    "3. Explore\n",
    "    1. 데이터를 분석하기 위해서는 통계적 지식이 필요하다. \n",
    "    \n",
    "    \n",
    "4. visualization\n",
    "    1. 명확성 \n",
    "    \n",
    "    \n",
    "    2. Brightness, saturation, Hue \n",
    "    \n",
    "    \n",
    "    3. The principal of Visualization: http://www.mbaexcel.com/excel/how-to-make-your-excel-bar-chart-look-better/\n",
    "        1. Data-Ink Ratio = Data ink / Total ink used in graphic \n",
    "            1. 되도록이면 3차원을 쓰지 말아라 (그림자 이용하지 말라)\n",
    "        2. The Lie Factor = Size of effect shown in graphic / size of effect in data \n",
    "            1. 실제보다 그림의 사이즈가 커지면 안된다. 정확한 정보전달이 아니다. \n",
    "        3. Reduce Chartjunk \n",
    "            1. 배경색깔, 위 테두리, 최대한 줄여라 \n",
    "        4. Scale distortion \n",
    "    4. Bar vs Lines\n",
    "        1. Lines imply connections - do not use for categorical data\n",
    "    5. Aspect Ratios\n",
    "        1. Baking to 45 degree\n",
    "        2. Cleveland proposes setting the aspect ratio so that the average of the absolute values of the orientations is 45 degrees. This is called \"banking the average orientation to 45 degrees\" or the AO (absolute orientation) method.2016. 1. 20.\n",
    "    7. Scatter Plot \n",
    "        1. Dot size(데이터가 많으면 size를 줄이거나 Opacity 색깔을 연하게 하면 된다. \n",
    "        2. 2D data\n",
    "        3. 3d data 일 때 3D scatter plot은 이용하지 말 것 \n",
    "    8. Bubble Chart \n",
    "        1. 3D data \n",
    "        2. 색깔이나 dot의 모양 변화를 통해 제 3의 data를 표현한다. \n",
    "    9. Pie chart \n",
    "        1. Composition \n",
    "    10. Bar charts\n",
    "        1. Compared\n",
    "    11. Donut은 쓰지 마라 각도의 정보가 사라진다. \n",
    "    12. Stacked Bar chart \n",
    "    13. Stacked Area Chart VS Line \n",
    "    14. Distribution\n",
    "        1. Hist\n",
    "        2. Density\n",
    "    ![15.](http://www.perceptualedge.com/blog/wp-content/uploads/2015/07/Abelas-Chart-Selection-Diagram.jpg) \n",
    "\n",
    "\n",
    "## Machine Learning \n",
    "### Modeling \n",
    "1. Ocaam;s Razor (보다 적은 수의 논리로 설명이 가능한 경우, 많은 수의 논리를 세우지 말라.)\n",
    "2. Bias/variance tradeoff: Overfitting high variance unseen data에 대해서는 예측을 하지 못한다. \n",
    "\n",
    "\n",
    "- BlackBox 딥러닝 같은 경우에는 이 안에가 어떻게 공부를 하였는지 알 수없다. \n",
    "- Descriptive Models Linear같은 경우에는 어떤 식으로 모델링되어있는지 확인할 수 있다. -> 설명할 수 있는 AI에 대해서 연구를 하고 있는 중\n",
    "\n",
    "\n",
    "딥러닝 네트워크는 왜 그런 판단을 하는지 설명이 쉽지 않다. \n",
    "\n",
    "\n",
    "- General : \n",
    "- Ad Hoc Model: 도메인 하나에서만 돌아가고 다른 곳에서는 이용되지 않는다.\n",
    "\n",
    "\n",
    "- Baseline: \n",
    "    1.  Random할 때보다 내 모델이 훨씬 더 좋은 성능이 나와야 한다. \n",
    "\n",
    "- Evaluating Classifiers\n",
    "    1. TP, TN\n",
    "    2. FP, FN\n",
    "    3. Accuracy = (TP + TN) / ALL \n",
    "    4. 암 발생 가능성에 대해서 매우 적을 경우 정확도의 결과는 bias를 갖는다. \n",
    "    5. 그러므로 Precision, F-score 이용\n",
    "    6. __범주형 자료 분석 수업과 관련된 통계적 지식 이용__\n",
    "    7. ROC, specificity, sensitibity, MSE \n",
    "\n",
    "### Test Data set \n",
    "- 6:2:2: Training(학습) validation(모델 개발)  testing data(확인)\n",
    "\n",
    "\n",
    "\n",
    "## Linear Regression \n",
    "\n",
    "1. 관련 수업 \n",
    "    1. __선형분석__ \n",
    "    2. __회귀분석__\n",
    "\n",
    "\n",
    "변수를 늘려 Dimension 을 늘려서 기계 학습을 시킨다 \n",
    "\n",
    "\n",
    "Power Law 일 경우에는(굉장히 극 소수의 큰 outcome이 있을 때)  최대한 bell shape으로 변경(정규 분포화)한 후에 Z-score를 한다. \n",
    "\n",
    "\n",
    "- 랏쏘와 릿지: https://brunch.co.kr/@itschloe1/11\n",
    "- Elastic-Net\n",
    "\n",
    "\n",
    "## Logistic Regression \n",
    "- Classification Mode - 로지스틱 회귀 분석 \n",
    "- __범주형 자료 분석__\n",
    "- Decision boundary를 구할 수 있다.\n",
    "\n",
    "\n",
    "- In-balanced training class sizes -> balanced 맞춰야 한다. (소수 클래스를 얻기 위해서 자료를 찾는다. 많은 클래스의 수를 버리거나 소수 클래스에 가중치값을 준다. 소수 데이터를 복제해서 약간씩 noise 를 넣어서 학습을 시킨다. )\n",
    "- Multi classes: combination을 통해서 각 2개씩 따로 기계 학습 그런 다음에 제일 가능성이 높은 것을 찾는다.\n",
    "- Hierarchical \n",
    "\n",
    "\n",
    "### Supervised Learning \n",
    "1. KNN: Nearest Neighbor \n",
    "\n",
    "\n",
    "2. 거리를 어떻게 정의할 것인지를 선택하면 된다.\n",
    "\n",
    "\n",
    "3. 기계 학습이 의미가 없다 \n",
    "\n",
    "\n",
    "4. 실제 inference할 때 모든 데이터의 거리를 계산하기 때문에 계산에 시간이 거린다.\n",
    "\n",
    "\n",
    "\n",
    "### Unsupervised Learning \n",
    "1. Clustering K-Means(clustering 을 k 개로 결정)\n",
    "\n",
    "\n",
    "2. Agglomerative Clustering: Hierarchical\n",
    "\n",
    "\n",
    "3. 클러스트들의 거리를 어떤 것을 구하느냐는 일반적으로 central, average 를 통해서 구한다. \n",
    "\n",
    "\n",
    "4. Decision Tree Classifiers \n",
    "    1. Information theoretic entropy: 쪼개기 전의 entropy - 쪼갠 후 entropy를 통해 제일 큰 것을 찾는다. \n",
    "    \n",
    "    \n",
    "5. Ensembles of Decision Trees\n",
    "    1. Begging \n",
    "    2. Boosting \n",
    "        1. Gradient boosted decision trees \n",
    "        \n",
    "        \n",
    "6. Bayesian Classifiers \n",
    "\n",
    "\n",
    "7. SVM: support vector machine 데이터가 충분하지 않다면 \n",
    "    1. 고차원으로 갈수록 훨씬 분류하기가 편해진다는 이론을 이용한다.\n",
    "    \n",
    "    \n",
    "8. Feature Engineering \n",
    "    1. Feature를 생성하거나 불필요한 Feature는 제거한다.\n",
    "    \n",
    "    \n",
    "9. Deep Learning\n",
    "    1. Backpropagation\n",
    "    2. 수 백만개의 parameter 중에서 제일 의미있는 것을 찾는다.\n",
    "\n",
    "\n",
    "- Dealing whit Big data\n",
    "- MapReduce - Hadoop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
